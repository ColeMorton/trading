"""
Validation Template Generator

Provides testing and validation templates for generated strategies.
"""

from .config_template import TemplateConfig


class ValidationTemplate:
    """Generates validation and testing templates for strategies."""

    def __init__(self, template_config: TemplateConfig):
        self.config = template_config

    def generate_test_file(self) -> str:
        """Generate comprehensive test file for the strategy."""
        strategy_name = self.config.strategy_name
        class_name = strategy_name.replace("_", "").title()

        return f'''"""
Test Suite for {class_name} Strategy

Comprehensive tests for {strategy_name} strategy including configuration validation,
signal generation, portfolio analysis, and integration tests.

Generated by Strategy Template Generator.
"""

import pytest
import polars as pl
import pandas as pd
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any

from app.strategies.{strategy_name}.config_types import DEFAULT_CONFIG, {class_name}Config
from app.strategies.{strategy_name}.tools.strategy_execution import (
    execute_strategy,
    calculate_indicators,
    generate_signals,
    validate_strategy_config
)
from app.strategies.{strategy_name}.exceptions import (
    {class_name}Error,
    {class_name}ConfigurationError,
    {class_name}ExecutionError,
    {class_name}PortfolioError
)


class Test{class_name}Configuration:
    """Test configuration validation and management."""

    def test_default_config_structure(self):
        """Test that default configuration has required fields."""
        config = DEFAULT_CONFIG

        # Required fields
        assert "TICKER" in config
        assert "BASE_DIR" in config
        assert "REFRESH" in config
        assert "DIRECTION" in config
        assert "USE_CURRENT" in config

        # Strategy-specific fields
{self._generate_config_field_tests()}

    def test_config_validation_valid(self):
        """Test configuration validation with valid config."""
        config = DEFAULT_CONFIG.copy()

        # Should not raise exception
        assert validate_strategy_config(config) is True

    def test_config_validation_invalid_ticker(self):
        """Test configuration validation with invalid ticker."""
        config = DEFAULT_CONFIG.copy()
        config["TICKER"] = None

        with pytest.raises(ValueError, match="TICKER is required"):
            validate_strategy_config(config)

    def test_config_validation_invalid_direction(self):
        """Test configuration validation with invalid direction."""
        config = DEFAULT_CONFIG.copy()
        config["DIRECTION"] = "Invalid"

        with pytest.raises(ValueError, match="DIRECTION must be Long or Short"):
            validate_strategy_config(config)

{self._generate_strategy_specific_config_tests()}


class Test{class_name}Indicators:
    """Test technical indicator calculations."""

    def setup_method(self):
        """Set up test data."""
        # Create sample price data
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        prices = 100 + (pd.Series(range(100)) * 0.1) + (pd.Series(range(100)) % 10 * 0.5)

        self.sample_data = pl.DataFrame({{
            'Date': dates,
            'Open': prices,
            'High': prices + 1,
            'Low': prices - 1,
            'Close': prices,
            'Volume': [1000] * 100
        }})

    def test_calculate_indicators_basic(self):
        """Test basic indicator calculation."""
        config = DEFAULT_CONFIG.copy()
        result = calculate_indicators(self.sample_data, config)

        # Should have original columns plus indicators
        assert len(result.columns) > len(self.sample_data.columns)

{self._generate_indicator_tests()}

    def test_calculate_indicators_insufficient_data(self):
        """Test indicator calculation with insufficient data."""
        # Create data with only 5 rows
        short_data = self.sample_data.head(5)
        config = DEFAULT_CONFIG.copy()

        # Should handle insufficient data gracefully
        result = calculate_indicators(short_data, config)
        assert len(result) == 5


class Test{class_name}Signals:
    """Test signal generation logic."""

    def setup_method(self):
        """Set up test data with indicators."""
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        prices = 100 + (pd.Series(range(100)) * 0.1)

        self.sample_data = pl.DataFrame({{
            'Date': dates,
            'Open': prices,
            'High': prices + 1,
            'Low': prices - 1,
            'Close': prices,
            'Volume': [1000] * 100
        }})

        # Add pre-calculated indicators for testing
{self._generate_test_indicator_data()}

    def test_generate_signals_long(self):
        """Test signal generation for long direction."""
        config = DEFAULT_CONFIG.copy()
        config["DIRECTION"] = "Long"

        result = generate_signals(self.sample_data, config)

        # Should have signal columns
        assert "Entry_Signal" in result.columns
        assert "Exit_Signal" in result.columns

        # Check signal data types
        assert result["Entry_Signal"].dtype == pl.Boolean
        assert result["Exit_Signal"].dtype == pl.Boolean

    def test_generate_signals_short(self):
        """Test signal generation for short direction."""
        config = DEFAULT_CONFIG.copy()
        config["DIRECTION"] = "Short"

        result = generate_signals(self.sample_data, config)

        # Should have signal columns
        assert "Entry_Signal" in result.columns
        assert "Exit_Signal" in result.columns

{self._generate_signal_tests()}


class Test{class_name}Execution:
    """Test strategy execution."""

    @patch('app.strategies.{strategy_name}.tools.strategy_execution.SignalProcessorFactory')
    def test_execute_strategy_success(self, mock_factory):
        """Test successful strategy execution."""
        # Mock signal processor
        mock_processor = Mock()
        mock_processor.process_current_signals.return_value = pl.DataFrame({{"signal": [1, 0, 1]}})
        mock_processor.process_ticker_portfolios.return_value = pl.DataFrame({{"portfolio": [1, 2, 3]}})
        mock_factory.create_processor.return_value = mock_processor

        # Mock log function
        mock_log = Mock()

        config = DEFAULT_CONFIG.copy()
        result = execute_strategy(config, "{strategy_name.upper()}", mock_log)

        assert result is not None
        assert len(result) == 3

    @patch('app.strategies.{strategy_name}.tools.strategy_execution.SignalProcessorFactory')
    def test_execute_strategy_no_portfolios(self, mock_factory):
        """Test strategy execution with no portfolios generated."""
        # Mock signal processor returning None
        mock_processor = Mock()
        mock_processor.process_ticker_portfolios.return_value = None
        mock_factory.create_processor.return_value = mock_processor

        mock_log = Mock()

        config = DEFAULT_CONFIG.copy()
        result = execute_strategy(config, "{strategy_name.upper()}", mock_log)

        assert result is None

    @patch('app.strategies.{strategy_name}.tools.strategy_execution.SignalProcessorFactory')
    def test_execute_strategy_error_handling(self, mock_factory):
        """Test strategy execution error handling."""
        # Mock signal processor that raises exception
        mock_processor = Mock()
        mock_processor.process_ticker_portfolios.side_effect = Exception("Test error")
        mock_factory.create_processor.return_value = mock_processor

        mock_log = Mock()

        config = DEFAULT_CONFIG.copy()

        with pytest.raises(Exception):
            execute_strategy(config, "{strategy_name.upper()}", mock_log)


class Test{class_name}Integration:
    """Integration tests for complete workflow."""

    @patch('app.strategies.{strategy_name}.1_get_portfolios.PortfolioOrchestrator')
    def test_run_function_success(self, mock_orchestrator):
        """Test main run function."""
        from app.strategies.{strategy_name}.1_get_portfolios import run

        # Mock orchestrator
        mock_instance = Mock()
        mock_instance.run.return_value = True
        mock_orchestrator.return_value = mock_instance

        config = DEFAULT_CONFIG.copy()
        result = run(config)

        assert result is True
        mock_orchestrator.assert_called_once()

    def test_exception_handling(self):
        """Test exception handling in main functions."""
        from app.strategies.{strategy_name}.exceptions import {class_name}Error

        # Test basic exception creation
        error = {class_name}Error("Test error")
        assert str(error) == "Test error"
        assert hasattr(error, 'error_code')
        assert hasattr(error, 'severity')

    def test_configuration_error(self):
        """Test configuration-specific error handling."""
        from app.strategies.{strategy_name}.exceptions import {class_name}ConfigurationError

        error = {class_name}ConfigurationError("Invalid config")
        assert "Configuration error" in str(error)


class Test{class_name}Performance:
    """Performance and edge case tests."""

    def test_large_dataset_handling(self):
        """Test strategy with large dataset."""
        # Create large dataset
        dates = pd.date_range('2020-01-01', periods=1000, freq='D')
        prices = 100 + (pd.Series(range(1000)) * 0.01)

        large_data = pl.DataFrame({{
            'Date': dates,
            'Open': prices,
            'High': prices + 1,
            'Low': prices - 1,
            'Close': prices,
            'Volume': [1000] * 1000
        }})

        config = DEFAULT_CONFIG.copy()

        # Should handle large dataset without errors
        result = calculate_indicators(large_data, config)
        assert len(result) == 1000

    def test_missing_data_handling(self):
        """Test strategy with missing data."""
        # Create data with NaN values
        dates = pd.date_range('2023-01-01', periods=50, freq='D')
        prices = [100.0] * 50
        prices[10:15] = [None] * 5  # Add missing values

        data_with_missing = pl.DataFrame({{
            'Date': dates,
            'Open': prices,
            'High': prices,
            'Low': prices,
            'Close': prices,
            'Volume': [1000] * 50
        }})

        config = DEFAULT_CONFIG.copy()

        # Should handle missing data gracefully
        result = calculate_indicators(data_with_missing, config)
        assert len(result) == 50


if __name__ == "__main__":
    pytest.main([__file__])
'''


    def _generate_config_field_tests(self) -> str:
        """Generate configuration field tests based on strategy type."""
        lines = []
        indent = "        "

        for key in self.config.config_fields:
            if key in ["TICKER", "BASE_DIR", "REFRESH", "DIRECTION", "USE_CURRENT"]:
                continue  # Already tested above

            lines.append(f'{indent}assert "{key}" in config')

        return "\n".join(lines)

    def _generate_strategy_specific_config_tests(self) -> str:
        """Generate strategy-specific configuration tests."""
        lines = []
        indent = "    "

        if self.config.strategy_type.value == "moving_average":
            lines.extend(
                [
                    f"{indent}def test_config_validation_invalid_windows(self):",
                    f'{indent}    """Test configuration validation with invalid window sizes."""',
                    f"{indent}    config = DEFAULT_CONFIG.copy()",
                    f'{indent}    config["FAST_PERIOD"] = 50',
                    f'{indent}    config["SLOW_PERIOD"] = 20',
                    f"{indent}    ",
                    f'{indent}    with pytest.raises(ValueError, match="Fast period must be less than slow period"):',
                    f"{indent}        validate_strategy_config(config)",
                    f"{indent}",
                    f"{indent}def test_config_validation_zero_windows(self):",
                    f'{indent}    """Test configuration validation with zero window sizes."""',
                    f"{indent}    config = DEFAULT_CONFIG.copy()",
                    f'{indent}    config["FAST_PERIOD"] = 0',
                    f"{indent}    ",
                    f'{indent}    with pytest.raises(ValueError, match="Window sizes must be positive"):',
                    f"{indent}        validate_strategy_config(config)",
                ],
            )

        elif self.config.strategy_type.value == "rsi":
            lines.extend(
                [
                    f"{indent}def test_config_validation_invalid_rsi_thresholds(self):",
                    f'{indent}    """Test configuration validation with invalid RSI thresholds."""',
                    f"{indent}    config = DEFAULT_CONFIG.copy()",
                    f'{indent}    config["RSI_OVERBOUGHT"] = 30',
                    f'{indent}    config["RSI_OVERSOLD"] = 70',
                    f"{indent}    ",
                    f'{indent}    with pytest.raises(ValueError, match="Invalid RSI thresholds"):',
                    f"{indent}        validate_strategy_config(config)",
                ],
            )

        return "\n".join(lines)

    def _generate_indicator_tests(self) -> str:
        """Generate indicator-specific tests."""
        lines = []
        indent = "        "

        if self.config.strategy_type.value == "moving_average":
            lines.extend(
                [
                    f"{indent}# Should have moving average columns",
                    f"{indent}assert 'SMA_Short' in result.columns",
                    f"{indent}assert 'SMA_Long' in result.columns",
                    f"{indent}assert 'EMA_Short' in result.columns",
                    f"{indent}assert 'EMA_Long' in result.columns",
                    f"{indent}",
                    f"{indent}# Check data types",
                    f"{indent}assert result['SMA_Short'].dtype in [pl.Float32, pl.Float64]",
                    f"{indent}assert result['SMA_Long'].dtype in [pl.Float32, pl.Float64]",
                ],
            )

        elif self.config.strategy_type.value == "rsi":
            lines.extend(
                [
                    f"{indent}# Should have RSI column",
                    f"{indent}assert 'RSI' in result.columns",
                    f"{indent}",
                    f"{indent}# Check RSI values are in valid range",
                    f"{indent}rsi_values = result['RSI'].drop_nulls()",
                    f"{indent}assert all(0 <= val <= 100 for val in rsi_values)",
                ],
            )

        elif self.config.strategy_type.value == "macd":
            lines.extend(
                [
                    f"{indent}# Should have MACD columns",
                    f"{indent}assert 'MACD_Line' in result.columns",
                    f"{indent}assert 'MACD_Signal' in result.columns",
                    f"{indent}assert 'MACD_Histogram' in result.columns",
                ],
            )

        else:
            lines.extend(
                [
                    f"{indent}# Should have custom indicator columns",
                    f"{indent}assert 'Momentum' in result.columns",
                ],
            )

        return "\n".join(lines)

    def _generate_test_indicator_data(self) -> str:
        """Generate test indicator data setup."""
        lines = []
        indent = "        "

        if self.config.strategy_type.value == "moving_average":
            lines.extend(
                [
                    f"{indent}# Add moving averages for testing",
                    f"{indent}short_ma = [None] * 19 + list(range(20, 81))  # 20-period SMA simulation",
                    f"{indent}long_ma = [None] * 49 + list(range(50, 51))   # 50-period SMA simulation",
                    f"{indent}",
                    f"{indent}self.sample_data = self.sample_data.with_columns([",
                    f"{indent}    pl.Series('SMA_Short', short_ma),",
                    f"{indent}    pl.Series('SMA_Long', long_ma),",
                    f"{indent}    pl.Series('EMA_Short', short_ma),",
                    f"{indent}    pl.Series('EMA_Long', long_ma),",
                    f"{indent}])",
                ],
            )

        elif self.config.strategy_type.value == "rsi":
            lines.extend(
                [
                    f"{indent}# Add RSI for testing",
                    f"{indent}rsi_values = [None] * 13 + [30 + (i % 40) for i in range(87)]  # RSI simulation",
                    f"{indent}",
                    f"{indent}self.sample_data = self.sample_data.with_columns([",
                    f"{indent}    pl.Series('RSI', rsi_values),",
                    f"{indent}])",
                ],
            )

        else:
            lines.extend(
                [
                    f"{indent}# Add custom indicators for testing",
                    f"{indent}momentum_values = [(i % 20 - 10) * 0.01 for i in range(100)]",
                    f"{indent}",
                    f"{indent}self.sample_data = self.sample_data.with_columns([",
                    f"{indent}    pl.Series('Momentum', momentum_values),",
                    f"{indent}])",
                ],
            )

        return "\n".join(lines)

    def _generate_signal_tests(self) -> str:
        """Generate signal-specific tests."""
        lines = []
        indent = "    "

        if self.config.strategy_type.value == "moving_average":
            lines.extend(
                [
                    f"{indent}def test_signal_crossover_detection(self):",
                    f'{indent}    """Test crossover signal detection."""',
                    f"{indent}    config = DEFAULT_CONFIG.copy()",
                    f'{indent}    config["DIRECTION"] = "Long"',
                    f"{indent}    ",
                    f"{indent}    result = generate_signals(self.sample_data, config)",
                    f"{indent}    ",
                    f"{indent}    # Should detect some signals",
                    f'{indent}    entry_signals = result["Entry_Signal"].sum()',
                    f'{indent}    exit_signals = result["Exit_Signal"].sum()',
                    f"{indent}    ",
                    f"{indent}    # In trending data, should have at least one entry signal",
                    f"{indent}    assert entry_signals >= 0",
                ],
            )

        elif self.config.strategy_type.value == "rsi":
            lines.extend(
                [
                    f"{indent}def test_rsi_threshold_signals(self):",
                    f'{indent}    """Test RSI threshold-based signals."""',
                    f"{indent}    config = DEFAULT_CONFIG.copy()",
                    f'{indent}    config["DIRECTION"] = "Long"',
                    f'{indent}    config["RSI_OVERSOLD"] = 30',
                    f'{indent}    config["RSI_OVERBOUGHT"] = 70',
                    f"{indent}    ",
                    f"{indent}    result = generate_signals(self.sample_data, config)",
                    f"{indent}    ",
                    f"{indent}    # Should have entry and exit signals",
                    f'{indent}    assert "Entry_Signal" in result.columns',
                    f'{indent}    assert "Exit_Signal" in result.columns',
                ],
            )

        return "\n".join(lines)

    def generate_benchmark_file(self) -> str:
        """Generate performance benchmark file."""
        strategy_name = self.config.strategy_name
        class_name = strategy_name.replace("_", "").title()

        return f'''"""
Performance Benchmarks for {class_name} Strategy

Benchmarks to ensure strategy performance meets expectations.

Generated by Strategy Template Generator.
"""

import time
import pytest
import polars as pl
import pandas as pd
from typing import Dict, Any

from app.strategies.{strategy_name}.tools.strategy_execution import (
    calculate_indicators,
    generate_signals,
    execute_strategy
)
from app.strategies.{strategy_name}.config_types import DEFAULT_CONFIG


class TestPerformanceBenchmarks:
    """Performance benchmarks for {strategy_name} strategy."""

    def setup_method(self):
        """Set up benchmark data."""
        # Create large dataset for performance testing
        dates = pd.date_range('2020-01-01', periods=2000, freq='D')
        prices = 100 + (pd.Series(range(2000)) * 0.01) + (pd.Series(range(2000)) % 50 * 0.1)

        self.large_data = pl.DataFrame({{
            'Date': dates,
            'Open': prices,
            'High': prices + 1,
            'Low': prices - 1,
            'Close': prices,
            'Volume': [100000] * 2000
        }})

        self.config = DEFAULT_CONFIG.copy()

    def test_indicator_calculation_performance(self):
        """Benchmark indicator calculation performance."""
        start_time = time.time()

        result = calculate_indicators(self.large_data, self.config)

        end_time = time.time()
        execution_time = end_time - start_time

        # Should complete within reasonable time (< 5 seconds for 2000 rows)
        assert execution_time < 5.0, f"Indicator calculation took {{execution_time:.2f}} seconds"
        assert len(result) == 2000

        print(f"Indicator calculation: {{execution_time:.3f}} seconds for {{len(self.large_data)}} rows")

    def test_signal_generation_performance(self):
        """Benchmark signal generation performance."""
        # First calculate indicators
        data_with_indicators = calculate_indicators(self.large_data, self.config)

        start_time = time.time()

        result = generate_signals(data_with_indicators, self.config)

        end_time = time.time()
        execution_time = end_time - start_time

        # Should complete within reasonable time (< 2 seconds for 2000 rows)
        assert execution_time < 2.0, f"Signal generation took {{execution_time:.2f}} seconds"
        assert len(result) == 2000

        print(f"Signal generation: {{execution_time:.3f}} seconds for {{len(data_with_indicators)}} rows")

    def test_memory_usage(self):
        """Test memory usage during processing."""
        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Process data
        result = calculate_indicators(self.large_data, self.config)
        result = generate_signals(result, self.config)

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory

        # Should not use excessive memory (< 100MB increase for 2000 rows)
        assert memory_increase < 100, f"Memory usage increased by {{memory_increase:.1f}} MB"

        print(f"Memory usage: {{memory_increase:.1f}} MB increase")

    def test_multiple_ticker_performance(self):
        """Test performance with multiple tickers."""
        tickers = ["AAPL", "GOOGL", "MSFT", "TSLA", "AMZN"]
        config = self.config.copy()
        config["TICKER"] = tickers

        start_time = time.time()

        # Simulate processing multiple tickers
        for ticker in tickers:
            ticker_config = config.copy()
            ticker_config["TICKER"] = ticker

            # Process each ticker
            result = calculate_indicators(self.large_data, ticker_config)
            result = generate_signals(result, ticker_config)

        end_time = time.time()
        execution_time = end_time - start_time

        # Should process all tickers within reasonable time
        avg_time_per_ticker = execution_time / len(tickers)
        assert avg_time_per_ticker < 10.0, f"Average time per ticker: {{avg_time_per_ticker:.2f}} seconds"

        print(f"Multiple ticker processing: {{execution_time:.2f}} seconds for {{len(tickers)}} tickers")

    def test_edge_case_performance(self):
        """Test performance with edge cases."""
        # Test with minimal data
        minimal_data = self.large_data.head(50)

        start_time = time.time()
        result = calculate_indicators(minimal_data, self.config)
        result = generate_signals(result, self.config)
        end_time = time.time()

        minimal_time = end_time - start_time
        assert minimal_time < 1.0, f"Minimal data processing took {{minimal_time:.2f}} seconds"

        # Test with data containing NaN values
        data_with_nans = self.large_data.clone()
        # Introduce some NaN values
        close_values = data_with_nans['Close'].to_list()
        close_values[100:110] = [None] * 10
        data_with_nans = data_with_nans.with_columns(pl.Series('Close', close_values))

        start_time = time.time()
        result = calculate_indicators(data_with_nans, self.config)
        result = generate_signals(result, self.config)
        end_time = time.time()

        nan_time = end_time - start_time
        assert nan_time < 5.0, f"NaN data processing took {{nan_time:.2f}} seconds"

        print(f"Edge case performance - Minimal: {{minimal_time:.3f}}s, NaN: {{nan_time:.3f}}s")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
'''

