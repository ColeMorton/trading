{
  "timestamp": "2025-06-02 16:36:01",
  "total_tests": 7,
  "passed": 1,
  "failed": 6,
  "elapsed_time": 27.046905994415283,
  "results": [
    {
      "description": "Tools Module Tests",
      "command": "python -m pytest tests/tools/ -v --tb=short",
      "success": false,
      "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pyproject.toml\nplugins: anyio-3.7.1\ncollecting ... collected 51 items\n\ntests/tools/test_error_handling.py::TestErrorHandling::test_convenience_functions FAILED [  1%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_handle_calculation_error FAILED [  3%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_result_class FAILED [  5%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_config_failure FAILED [  7%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_config_success FAILED [  9%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_dataframe_failure FAILED [ 11%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_dataframe_success FAILED [ 13%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_numeric_array_failure FAILED [ 15%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_numeric_array_success FAILED [ 17%]\ntests/tools/test_error_handling.py::TestErrorHandling::test_with_error_handling_decorator FAILED [ 19%]\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy PASSED [ 21%]\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_from_returns PASSED [ 23%]\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_metrics PASSED [ 25%]\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_per_month PASSED [ 27%]\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_with_stop_loss PASSED [ 29%]\ntests/tools/test_expectancy_integration.py::TestExpectancyIntegration::test_expectancy_consistency PASSED [ 31%]\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_empty_data FAILED [ 33%]\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_find_best_horizon FAILED [ 35%]\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_no_forward_looking_bias FAILED [ 37%]\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_no_signals PASSED [ 39%]\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_position_based_evaluation FAILED [ 41%]\ntests/tools/test_normalization.py::TestNormalization::test_convenience_functions FAILED [ 43%]\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_edge_cases FAILED [ 45%]\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_list FAILED [ 47%]\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_numpy FAILED [ 49%]\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_pandas FAILED [ 50%]\ntests/tools/test_normalization.py::TestNormalization::test_normalize_dataframe FAILED [ 52%]\ntests/tools/test_normalization.py::TestNormalization::test_normalize_metrics FAILED [ 54%]\ntests/tools/test_normalization.py::TestNormalization::test_robust_scale FAILED [ 56%]\ntests/tools/test_normalization.py::TestNormalization::test_z_score_normalize FAILED [ 58%]\ntests/tools/test_signal_conversion.py::TestSignalConversion::test_convert_signals_to_positions_pandas PASSED [ 60%]\ntests/tools/test_signal_conversion.py::TestSignalConversion::test_convert_signals_to_positions_polars PASSED [ 62%]\ntests/tools/test_signal_conversion.py::TestSignalConversion::test_rsi_filter PASSED [ 64%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_calculate_frequency_metrics PASSED [ 66%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_calculate_portfolio_metrics PASSED [ 68%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_calculate_quality_metrics PASSED [ 70%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_empty_data PASSED [ 72%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_find_best_horizon PASSED [ 74%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_horizon_metrics PASSED [ 76%]\ntests/tools/test_signal_metrics.py::TestSignalMetrics::test_legacy_functions PASSED [ 78%]\ntests/tools/test_signal_quality.py::TestSignalQualityMetrics::test_calculate_signal_quality_metrics PASSED [ 80%]\ntests/tools/test_signal_quality.py::TestSignalQualityMetrics::test_empty_data PASSED [ 82%]\ntests/tools/test_signal_quality.py::TestSignalQualityMetrics::test_horizon_metrics_integration PASSED [ 84%]\ntests/tools/test_signal_quality.py::TestSignalQualityMetrics::test_no_signals PASSED [ 86%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_apply_stop_loss_to_returns PASSED [ 88%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_apply_stop_loss_to_signal_quality_metrics PASSED [ 90%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_calculate_stop_loss_adjusted_metrics PASSED [ 92%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_compare_stop_loss_levels PASSED [ 94%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_empty_data PASSED [ 96%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_find_optimal_stop_loss PASSED [ 98%]\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_invalid_stop_loss PASSED [100%]\n\n=================================== FAILURES ===================================\n_________________ TestErrorHandling.test_convenience_functions _________________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n_______________ TestErrorHandling.test_handle_calculation_error ________________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n_____________________ TestErrorHandling.test_result_class ______________________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n________________ TestErrorHandling.test_validate_config_failure ________________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n________________ TestErrorHandling.test_validate_config_success ________________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n______________ TestErrorHandling.test_validate_dataframe_failure _______________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n______________ TestErrorHandling.test_validate_dataframe_success _______________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n____________ TestErrorHandling.test_validate_numeric_array_failure _____________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n____________ TestErrorHandling.test_validate_numeric_array_success _____________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n_____________ TestErrorHandling.test_with_error_handling_decorator _____________\ntests/tools/test_error_handling.py:28: in setUp\n    self.error_handler = ErrorHandler()\napp/tools/error_handling.py:72: in __init__\n    self.log, _, _, _ = setup_logging(\"error_handler\", Path(\"./logs\"), \"error_handler.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'error_handler.log'\n_____________________ TestHorizonAnalysis.test_empty_data ______________________\ntests/tools/test_horizon_analysis.py:108: in test_empty_data\n    self.assertIsNone(best_horizon)\nE   AssertionError: 1 is not None\n__________________ TestHorizonAnalysis.test_find_best_horizon __________________\ntests/tools/test_horizon_analysis.py:87: in test_find_best_horizon\n    self.assertEqual(best_horizon, 3)\nE   AssertionError: 5 != 3\n_______________ TestHorizonAnalysis.test_no_forward_looking_bias _______________\ntests/tools/test_horizon_analysis.py:32: in test_no_forward_looking_bias\n    self.assertEqual(horizon_metrics[\"1\"][\"sample_size\"], 2)\nE   AssertionError: 7 != 2\n______________ TestHorizonAnalysis.test_position_based_evaluation ______________\ntests/tools/test_horizon_analysis.py:64: in test_position_based_evaluation\n    self.assertEqual(horizon_metrics[\"1\"][\"sample_size\"], 3)\nE   AssertionError: 7 != 3\n_________________ TestNormalization.test_convenience_functions _________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n_______________ TestNormalization.test_min_max_scale_edge_cases ________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n__________________ TestNormalization.test_min_max_scale_list ___________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n__________________ TestNormalization.test_min_max_scale_numpy __________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n_________________ TestNormalization.test_min_max_scale_pandas __________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n__________________ TestNormalization.test_normalize_dataframe __________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n___________________ TestNormalization.test_normalize_metrics ___________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n_____________________ TestNormalization.test_robust_scale ______________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n___________________ TestNormalization.test_z_score_normalize ___________________\ntests/tools/test_normalization.py:24: in setUp\n    self.normalizer = Normalizer()\napp/tools/normalization.py:32: in __init__\n    self.log, _, _, _ = setup_logging(\"normalizer\", Path(\"./logs\"), \"normalizer.log\")\napp/tools/setup_logging.py:58: in setup_logging\n    logger.setLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:1464: in setLevel\n    self.level = _checkLevel(level)\n../../.pyenv/versions/3.11.9/lib/python3.11/logging/__init__.py:207: in _checkLevel\n    raise ValueError(\"Unknown level: %r\" % level)\nE   ValueError: Unknown level: 'normalizer.log'\n=============================== warnings summary ===============================\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_empty_data\n  /Users/colemorton/.pyenv/versions/3.11.9/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n    return _methods._mean(a, axis=axis, dtype=dtype,\n\ntests/tools/test_stop_loss_simulator.py::TestStopLossSimulator::test_empty_data\n  /Users/colemorton/.pyenv/versions/3.11.9/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n    ret = ret.dtype.type(ret / rcount)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_convenience_functions\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_handle_calculation_error\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_result_class\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_validate_config_failure\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_validate_config_success\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_validate_dataframe_failure\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_validate_dataframe_success\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_validate_numeric_array_failure\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_validate_numeric_array_success\nFAILED tests/tools/test_error_handling.py::TestErrorHandling::test_with_error_handling_decorator\nFAILED tests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_empty_data\nFAILED tests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_find_best_horizon\nFAILED tests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_no_forward_looking_bias\nFAILED tests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_position_based_evaluation\nFAILED tests/tools/test_normalization.py::TestNormalization::test_convenience_functions\nFAILED tests/tools/test_normalization.py::TestNormalization::test_min_max_scale_edge_cases\nFAILED tests/tools/test_normalization.py::TestNormalization::test_min_max_scale_list\nFAILED tests/tools/test_normalization.py::TestNormalization::test_min_max_scale_numpy\nFAILED tests/tools/test_normalization.py::TestNormalization::test_min_max_scale_pandas\nFAILED tests/tools/test_normalization.py::TestNormalization::test_normalize_dataframe\nFAILED tests/tools/test_normalization.py::TestNormalization::test_normalize_metrics\nFAILED tests/tools/test_normalization.py::TestNormalization::test_robust_scale\nFAILED tests/tools/test_normalization.py::TestNormalization::test_z_score_normalize\n================== 23 failed, 28 passed, 2 warnings in 3.34s ===================\n"
    },
    {
      "description": "Strategy Module Tests",
      "command": "python -m pytest tests/strategies/ -v --tb=short -k not integration",
      "success": false,
      "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pyproject.toml\nplugins: anyio-3.7.1\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n______ ERROR collecting tests/strategies/ma_cross/test_core_components.py ______\nImportError while importing test module '/Users/colemorton/Projects/trading/tests/strategies/ma_cross/test_core_components.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\ntests/strategies/ma_cross/test_core_components.py:12: in <module>\n    from app.strategies.ma_cross.core.models import (\nE   ImportError: cannot import name 'MACrossConfig' from 'app.strategies.ma_cross.core.models' (/Users/colemorton/Projects/trading/app/strategies/ma_cross/core/models.py)\n=========================== short test summary info ============================\nERROR tests/strategies/ma_cross/test_core_components.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 2.66s ===============================\n"
    },
    {
      "description": "Concurrency Smoke Tests",
      "command": "python -m pytest tests/concurrency/test_smoke.py -v",
      "success": true,
      "output": ""
    },
    {
      "description": "Portfolio Orchestrator Tests",
      "command": "python -m pytest tests/test_portfolio_orchestrator.py -v --tb=short",
      "success": false,
      "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pyproject.toml\nplugins: anyio-3.7.1\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n____________ ERROR collecting tests/test_portfolio_orchestrator.py _____________\nImportError while importing test module '/Users/colemorton/Projects/trading/tests/test_portfolio_orchestrator.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\ntests/test_portfolio_orchestrator.py:12: in <module>\n    from app.tools.orchestration.portfolio_orchestrator import PortfolioOrchestrator\napp/tools/orchestration/__init__.py:8: in <module>\n    from .portfolio_orchestrator import PortfolioOrchestrator\napp/tools/orchestration/portfolio_orchestrator.py:35: in <module>\n    from app.ma_cross.tools.filter_portfolios import filter_portfolios\nE   ModuleNotFoundError: No module named 'app.ma_cross'\n=========================== short test summary info ============================\nERROR tests/test_portfolio_orchestrator.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 2.63s ===============================\n"
    },
    {
      "description": "Export Integration Tests",
      "command": "python -m pytest tests/test_export_integration.py -v --tb=short",
      "success": false,
      "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pyproject.toml\nplugins: anyio-3.7.1\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n______________ ERROR collecting tests/test_export_integration.py _______________\nImportError while importing test module '/Users/colemorton/Projects/trading/tests/test_export_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\ntests/test_export_integration.py:15: in <module>\n    from app.tools.orchestration.portfolio_orchestrator import PortfolioOrchestrator\napp/tools/orchestration/__init__.py:8: in <module>\n    from .portfolio_orchestrator import PortfolioOrchestrator\napp/tools/orchestration/portfolio_orchestrator.py:35: in <module>\n    from app.ma_cross.tools.filter_portfolios import filter_portfolios\nE   ModuleNotFoundError: No module named 'app.ma_cross'\n=========================== short test summary info ============================\nERROR tests/test_export_integration.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 3.75s ===============================\n"
    },
    {
      "description": "Strategy Integration Tests",
      "command": "python -m pytest tests/test_strategy_integration.py -v --tb=short",
      "success": false,
      "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pyproject.toml\nplugins: anyio-3.7.1\ncollecting ... collected 7 items\n\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_sma FAILED [ 14%]\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_ema FAILED [ 28%]\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_execute_single_strategy_with_factory PASSED [ 42%]\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_factory_available_strategies PASSED [ 57%]\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_backward_compatibility_with_string_parameter FAILED [ 71%]\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_config_override_strategy_type FAILED [ 85%]\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_error_handling_with_invalid_strategy PASSED [100%]\n\n=================================== FAILURES ===================================\n_ TestFactoryIntegrationWithExistingCode.test_calculate_ma_and_signals_with_sma _\ntests/test_strategy_integration.py:51: in test_calculate_ma_and_signals_with_sma\n    result = calculate_ma_and_signals(data, 10, 20, config, log)\napp/tools/calculate_ma_and_signals.py:38: in calculate_ma_and_signals\n    result = strategy.calculate(data, short_window, long_window, config, log)\napp/tools/strategy/concrete.py:52: in calculate\n    raise ValueError(\"Invalid data\")\nE   ValueError: Invalid data\n_ TestFactoryIntegrationWithExistingCode.test_calculate_ma_and_signals_with_ema _\ntests/test_strategy_integration.py:77: in test_calculate_ma_and_signals_with_ema\n    result = calculate_ma_and_signals(data, 12, 26, config, log, \"EMA\")\napp/tools/calculate_ma_and_signals.py:38: in calculate_ma_and_signals\n    result = strategy.calculate(data, short_window, long_window, config, log)\napp/tools/strategy/concrete.py:133: in calculate\n    raise ValueError(\"Invalid data\")\nE   ValueError: Invalid data\n_ TestFactoryIntegrationWithExistingCode.test_backward_compatibility_with_string_parameter _\ntests/test_strategy_integration.py:150: in test_backward_compatibility_with_string_parameter\n    result = calculate_ma_and_signals(data, 20, 50, config, log, \"SMA\")\napp/tools/calculate_ma_and_signals.py:38: in calculate_ma_and_signals\n    result = strategy.calculate(data, short_window, long_window, config, log)\napp/tools/strategy/concrete.py:52: in calculate\n    raise ValueError(\"Invalid data\")\nE   ValueError: Invalid data\n__ TestFactoryIntegrationWithExistingCode.test_config_override_strategy_type ___\ntests/test_strategy_integration.py:173: in test_config_override_strategy_type\n    result = calculate_ma_and_signals(data, 12, 26, config, log, \"SMA\")\napp/tools/calculate_ma_and_signals.py:38: in calculate_ma_and_signals\n    result = strategy.calculate(data, short_window, long_window, config, log)\napp/tools/strategy/concrete.py:133: in calculate\n    raise ValueError(\"Invalid data\")\nE   ValueError: Invalid data\n=========================== short test summary info ============================\nFAILED tests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_sma\nFAILED tests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_ema\nFAILED tests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_backward_compatibility_with_string_parameter\nFAILED tests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_config_override_strategy_type\n========================= 4 failed, 3 passed in 2.61s ==========================\n"
    },
    {
      "description": "API Unit Tests",
      "command": "python -m pytest tests/api/ -v --tb=short -k not server",
      "success": false,
      "output": "2025-06-02 16:36:01,059 - INFO - Logging initialized\n2025-06-02 16:36:01,060 - INFO - Starting execution\n2025-06-02 16:36:01,060 - INFO - API server logging initialized\n2025-06-02 16:36:01,063 - INFO - Logging initialized\n2025-06-02 16:36:01,064 - INFO - Starting execution\n2025-06-02 16:36:01,064 - INFO - API server logging initialized\n2025-06-02 16:36:01,067 - INFO - Logging initialized\n2025-06-02 16:36:01,067 - INFO - Starting execution\n2025-06-02 16:36:01,067 - INFO - API server logging initialized\n2025-06-02 16:36:01,068 - INFO - Logging initialized\n2025-06-02 16:36:01,068 - INFO - Starting execution\n2025-06-02 16:36:01,068 - INFO - API server logging initialized\n2025-06-02 16:36:01,068 - INFO - Logging initialized\n2025-06-02 16:36:01,068 - INFO - Starting execution\n2025-06-02 16:36:01,068 - INFO - API server logging initialized\nImportError while loading conftest '/Users/colemorton/Projects/trading/tests/api/conftest.py'.\ntests/api/conftest.py:15: in <module>\n    from app.api.main import app\napp/api/main.py:17: in <module>\n    from app.api.routers import scripts, data, viewer, sensylate, ma_cross, health\napp/api/routers/health.py:12: in <module>\n    from app.database.config import get_prisma, get_redis, get_database_manager\napp/database/config.py:11: in <module>\n    import asyncpg\nE   ModuleNotFoundError: No module named 'asyncpg'\n"
    }
  ]
}