{
  "timestamp": "2025-06-16 08:15:26",
  "total_tests": 7,
  "passed": 1,
  "failed": 6,
  "elapsed_time": 29.07971715927124,
  "results": [
    {
      "description": "Tools Module Tests",
      "command": "python -m pytest tests/tools/ -v --tb=short",
      "success": false,
      "output": "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pytest.ini\nplugins: timeout-2.4.0, cov-6.2.1, mock-3.14.1, anyio-3.7.1, asyncio-1.0.0\ntimeout: 300.0s\ntimeout method: thread\ntimeout func_only: False\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 58 items\n\ntests/tools/test_download_data_thread_safety.py::TestDownloadDataThreadSafety::test_concurrent_downloads_without_lock_causes_contamination \u001b[32mPASSED\u001b[0m\u001b[33m [  1%]\u001b[0m\ntests/tools/test_download_data_thread_safety.py::TestDownloadDataThreadSafety::test_concurrent_downloads_with_lock_prevents_contamination \u001b[32mPASSED\u001b[0m\u001b[33m [  3%]\u001b[0m\ntests/tools/test_download_data_thread_safety.py::TestDownloadDataThreadSafety::test_thread_lock_ensures_sequential_yfinance_calls \u001b[32mPASSED\u001b[0m\u001b[33m [  5%]\u001b[0m\ntests/tools/test_download_data_thread_safety.py::TestDownloadDataThreadSafety::test_error_handling_with_concurrent_downloads \u001b[32mPASSED\u001b[0m\u001b[33m [  6%]\u001b[0m\ntests/tools/test_download_data_thread_safety.py::TestDownloadDataThreadSafety::test_data_integrity_across_concurrent_downloads \u001b[32mPASSED\u001b[0m\u001b[33m [  8%]\u001b[0m\ntests/tools/test_download_data_thread_safety.py::TestDownloadDataThreadSafety::test_performance_impact_of_thread_lock \u001b[32mPASSED\u001b[0m\u001b[33m [ 10%]\u001b[0m\ntests/tools/test_download_data_thread_safety.py::TestConcurrentStrategyExecution::test_full_concurrent_pipeline_with_thread_safety \u001b[32mPASSED\u001b[0m\u001b[33m [ 12%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_convenience_functions \u001b[32mPASSED\u001b[0m\u001b[33m [ 13%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_handle_calculation_error \u001b[32mPASSED\u001b[0m\u001b[33m [ 15%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_result_class \u001b[32mPASSED\u001b[0m\u001b[33m [ 17%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_config_failure \u001b[32mPASSED\u001b[0m\u001b[33m [ 18%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_config_success \u001b[32mPASSED\u001b[0m\u001b[33m [ 20%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_dataframe_failure \u001b[32mPASSED\u001b[0m\u001b[33m [ 22%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_dataframe_success \u001b[32mPASSED\u001b[0m\u001b[33m [ 24%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_numeric_array_failure \u001b[32mPASSED\u001b[0m\u001b[33m [ 25%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_validate_numeric_array_success \u001b[32mPASSED\u001b[0m\u001b[33m [ 27%]\u001b[0m\ntests/tools/test_error_handling.py::TestErrorHandling::test_with_error_handling_decorator \u001b[32mPASSED\u001b[0m\u001b[33m [ 29%]\u001b[0m\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy \u001b[32mPASSED\u001b[0m\u001b[33m [ 31%]\u001b[0m\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_from_returns \u001b[32mPASSED\u001b[0m\u001b[33m [ 32%]\u001b[0m\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_metrics \u001b[32mPASSED\u001b[0m\u001b[33m [ 34%]\u001b[0m\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_per_month \u001b[32mPASSED\u001b[0m\u001b[33m [ 36%]\u001b[0m\ntests/tools/test_expectancy.py::TestExpectancy::test_calculate_expectancy_with_stop_loss \u001b[32mPASSED\u001b[0m\u001b[33m [ 37%]\u001b[0m\ntests/tools/test_expectancy_integration.py::TestExpectancyIntegration::test_expectancy_consistency \u001b[32mPASSED\u001b[0m\u001b[33m [ 39%]\u001b[0m\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_empty_data \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_find_best_horizon \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_no_forward_looking_bias \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_no_signals \u001b[32mPASSED\u001b[0m\u001b[31m [ 46%]\u001b[0m\ntests/tools/test_horizon_analysis.py::TestHorizonAnalysis::test_position_based_evaluation \u001b[31mFAILED\u001b[0m\u001b[31m [ 48%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_convenience_functions \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_edge_cases \u001b[31mFAILED\u001b[0m\u001b[31m [ 51%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_list \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_numpy \u001b[32mPASSED\u001b[0m\u001b[31m [ 55%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_min_max_scale_pandas \u001b[32mPASSED\u001b[0m\u001b[31m [ 56%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_normalize_dataframe \u001b[32mPASSED\u001b[0m\u001b[31m [ 58%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_normalize_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_robust_scale \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\ntests/tools/test_normalization.py::TestNormalization::test_z_score_normalize \u001b[31mFAILED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n\n=================================== FAILURES ===================================\n\u001b[31m\u001b[1m_____________________ TestHorizonAnalysis.test_empty_data ______________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_horizon_analysis.py\u001b[0m:119: in test_empty_data\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertIsNone(best_horizon)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: 1 is not None\u001b[0m\n\u001b[31m\u001b[1m__________________ TestHorizonAnalysis.test_find_best_horizon __________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_horizon_analysis.py\u001b[0m:96: in test_find_best_horizon\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertEqual(best_horizon, \u001b[94m3\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: 5 != 3\u001b[0m\n\u001b[31m\u001b[1m_______________ TestHorizonAnalysis.test_no_forward_looking_bias _______________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_horizon_analysis.py\u001b[0m:39: in test_no_forward_looking_bias\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertEqual(horizon_metrics[\u001b[33m\"\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33msample_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: 7 != 2\u001b[0m\n\u001b[31m\u001b[1m______________ TestHorizonAnalysis.test_position_based_evaluation ______________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_horizon_analysis.py\u001b[0m:73: in test_position_based_evaluation\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertEqual(horizon_metrics[\u001b[33m\"\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33msample_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[94m3\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: 7 != 3\u001b[0m\n\u001b[31m\u001b[1m_________________ TestNormalization.test_convenience_functions _________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_normalization.py\u001b[0m:201: in test_convenience_functions\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertTrue(\u001b[94m0\u001b[39;49;00m <= result[\u001b[33m\"\u001b[39;49;00m\u001b[33mprofit_factor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] <= \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: False is not true\u001b[0m\n----------------------------- Captured stderr call -----------------------------\n2025-06-16 08:15:04,484 - INFO - Logging initialized\n2025-06-16 08:15:04,485 - INFO - Starting execution\n2025-06-16 08:15:04,486 - INFO - Logging initialized\n2025-06-16 08:15:04,486 - INFO - Starting execution\n2025-06-16 08:15:04,496 - INFO - Logging initialized\n2025-06-16 08:15:04,497 - INFO - Starting execution\n2025-06-16 08:15:04,497 - INFO - Logging initialized\n2025-06-16 08:15:04,497 - INFO - Starting execution\n2025-06-16 08:15:04,497 - INFO - Normalizing metrics dictionary using min_max method\n2025-06-16 08:15:04,497 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,497 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,497 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,497 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,497 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,497 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,497 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,497 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,497 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,497 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n\u001b[31m\u001b[1m_______________ TestNormalization.test_min_max_scale_edge_cases ________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_normalization.py\u001b[0m:87: in test_min_max_scale_edge_cases\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertEqual(result, [\u001b[94m0.5\u001b[39;49;00m])  \u001b[90m# Midpoint of default range\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Lists differ: [5.0] != [0.5]\u001b[0m\n\u001b[1m\u001b[31mE   \u001b[0m\n\u001b[1m\u001b[31mE   First differing element 0:\u001b[0m\n\u001b[1m\u001b[31mE   5.0\u001b[0m\n\u001b[1m\u001b[31mE   0.5\u001b[0m\n\u001b[1m\u001b[31mE   \u001b[0m\n\u001b[1m\u001b[31mE   - [5.0]\u001b[0m\n\u001b[1m\u001b[31mE   + [0.5]\u001b[0m\n----------------------------- Captured stderr call -----------------------------\n2025-06-16 08:15:04,501 - INFO - Logging initialized\n2025-06-16 08:15:04,501 - INFO - Starting execution\n2025-06-16 08:15:04,502 - WARNING - Empty data provided for normalization\n2025-06-16 08:15:04,502 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,502 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n\u001b[31m\u001b[1m__________________ TestNormalization.test_min_max_scale_list ___________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_normalization.py\u001b[0m:72: in test_min_max_scale_list\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertEqual(result, expected)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Lists differ: [1.0, 2.0, 3.0, 4.0, 5.0] != [0.0, 0.25, 0.5, 0.75, 1.0]\u001b[0m\n\u001b[1m\u001b[31mE   \u001b[0m\n\u001b[1m\u001b[31mE   First differing element 0:\u001b[0m\n\u001b[1m\u001b[31mE   1.0\u001b[0m\n\u001b[1m\u001b[31mE   0.0\u001b[0m\n\u001b[1m\u001b[31mE   \u001b[0m\n\u001b[1m\u001b[31mE   - [1.0, 2.0, 3.0, 4.0, 5.0]\u001b[0m\n\u001b[1m\u001b[31mE   + [0.0, 0.25, 0.5, 0.75, 1.0]\u001b[0m\n----------------------------- Captured stderr call -----------------------------\n2025-06-16 08:15:04,506 - INFO - Logging initialized\n2025-06-16 08:15:04,506 - INFO - Starting execution\n2025-06-16 08:15:04,506 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n\u001b[31m\u001b[1m___________________ TestNormalization.test_normalize_metrics ___________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_normalization.py\u001b[0m:140: in test_normalize_metrics\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertTrue(\u001b[94m0\u001b[39;49;00m <= result[\u001b[33m\"\u001b[39;49;00m\u001b[33mprofit_factor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] <= \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: False is not true\u001b[0m\n----------------------------- Captured stderr call -----------------------------\n2025-06-16 08:15:04,520 - INFO - Logging initialized\n2025-06-16 08:15:04,520 - INFO - Starting execution\n2025-06-16 08:15:04,520 - INFO - Normalizing metrics dictionary using min_max method\n2025-06-16 08:15:04,520 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,520 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,520 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,520 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,520 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,520 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,520 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,521 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n2025-06-16 08:15:04,521 - WARNING - Single value or constant data provided for normalization\n2025-06-16 08:15:04,521 - ERROR - Error in min_max_scale: cannot access local variable 'is_series' where it is not associated with a value\n\u001b[31m\u001b[1m_____________________ TestNormalization.test_robust_scale ______________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_normalization.py\u001b[0m:123: in test_robust_scale\n    \u001b[0mnp.testing.assert_array_almost_equal(result, expected)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\u001b[0m:81: in inner\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\u001b[0m:81: in inner\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: \u001b[0m\n\u001b[1m\u001b[31mE   Arrays are not almost equal to 6 decimals\u001b[0m\n\u001b[1m\u001b[31mE   \u001b[0m\n\u001b[1m\u001b[31mE   Mismatched elements: 6 / 6 (100%)\u001b[0m\n\u001b[1m\u001b[31mE   Max absolute difference: 6.43333333\u001b[0m\n\u001b[1m\u001b[31mE   Max relative difference: 0.2\u001b[0m\n\u001b[1m\u001b[31mE    x: array([-1. , -0.6, -0.2,  0.2,  0.6, 38.6])\u001b[0m\n\u001b[1m\u001b[31mE    y: array([-0.833333, -0.5     , -0.166667,  0.166667,  0.5     , 32.166667])\u001b[0m\n----------------------------- Captured stderr call -----------------------------\n2025-06-16 08:15:04,524 - INFO - Logging initialized\n2025-06-16 08:15:04,524 - INFO - Starting execution\n\u001b[31m\u001b[1m___________________ TestNormalization.test_z_score_normalize ___________________\u001b[0m\n\u001b[1m\u001b[31mtests/tools/test_normalization.py\u001b[0m:110: in test_z_score_normalize\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.assertAlmostEqual(r, e, places=\u001b[94m6\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: 1.0 != -1.41421356 within 6 places (2.4142135600000003 difference)\u001b[0m\n----------------------------- Captured stderr call -----------------------------\n2025-06-16 08:15:04,535 - INFO - Logging initialized\n2025-06-16 08:15:04,535 - INFO - Starting execution\n2025-06-16 08:15:04,535 - ERROR - Error in z_score_normalize: cannot access local variable 'is_series' where it is not associated with a value\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mFAILED\u001b[0m tests/tools/test_horizon_analysis.py::\u001b[1mTestHorizonAnalysis::test_empty_data\u001b[0m - AssertionError: 1 is not None\n\u001b[31mFAILED\u001b[0m tests/tools/test_horizon_analysis.py::\u001b[1mTestHorizonAnalysis::test_find_best_horizon\u001b[0m - AssertionError: 5 != 3\n\u001b[31mFAILED\u001b[0m tests/tools/test_horizon_analysis.py::\u001b[1mTestHorizonAnalysis::test_no_forward_looking_bias\u001b[0m - AssertionError: 7 != 2\n\u001b[31mFAILED\u001b[0m tests/tools/test_horizon_analysis.py::\u001b[1mTestHorizonAnalysis::test_position_based_evaluation\u001b[0m - AssertionError: 7 != 3\n\u001b[31mFAILED\u001b[0m tests/tools/test_normalization.py::\u001b[1mTestNormalization::test_convenience_functions\u001b[0m - AssertionError: False is not true\n\u001b[31mFAILED\u001b[0m tests/tools/test_normalization.py::\u001b[1mTestNormalization::test_min_max_scale_edge_cases\u001b[0m - AssertionError: Lists differ: [5.0] != [0.5]\n\nFirst differing element 0:\n5.0\n0.5\n\n- [5.0]\n+ [0.5]\n\u001b[31mFAILED\u001b[0m tests/tools/test_normalization.py::\u001b[1mTestNormalization::test_min_max_scale_list\u001b[0m - AssertionError: Lists differ: [1.0, 2.0, 3.0, 4.0, 5.0] != [0.0, 0.25, 0.5, 0.75, 1.0]\n\nFirst differing element 0:\n1.0\n0.0\n\n- [1.0, 2.0, 3.0, 4.0, 5.0]\n+ [0.0, 0.25, 0.5, 0.75, 1.0]\n\u001b[31mFAILED\u001b[0m tests/tools/test_normalization.py::\u001b[1mTestNormalization::test_normalize_metrics\u001b[0m - AssertionError: False is not true\n\u001b[31mFAILED\u001b[0m tests/tools/test_normalization.py::\u001b[1mTestNormalization::test_robust_scale\u001b[0m - AssertionError: \nArrays are not almost equal to 6 decimals\n\nMismatched elements: 6 / 6 (100%)\nMax absolute difference: 6.43333333\nMax relative difference: 0.2\n x: array([-1. , -0.6, -0.2,  0.2,  0.6, 38.6])\n y: array([-0.833333, -0.5     , -0.166667,  0.166667,  0.5     , 32.166667])\n\u001b[31mFAILED\u001b[0m tests/tools/test_normalization.py::\u001b[1mTestNormalization::test_z_score_normalize\u001b[0m - AssertionError: 1.0 != -1.41421356 within 6 places (2.4142135600000003 difference)\n\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 10 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n\u001b[31m================== \u001b[31m\u001b[1m10 failed\u001b[0m, \u001b[32m27 passed\u001b[0m, \u001b[33m12 warnings\u001b[0m\u001b[31m in 3.11s\u001b[0m\u001b[31m ==================\u001b[0m\n"
    },
    {
      "description": "Strategy Module Tests",
      "command": "python -m pytest tests/strategies/ -v --tb=short -k not integration",
      "success": false,
      "output": "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pytest.ini\nplugins: timeout-2.4.0, cov-6.2.1, mock-3.14.1, anyio-3.7.1, asyncio-1.0.0\ntimeout: 300.0s\ntimeout method: thread\ntimeout func_only: False\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 58 items / 2 deselected / 56 selected\n\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_create_ticker_batches_small_list \u001b[32mPASSED\u001b[0m\u001b[33m [  1%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_create_ticker_batches_medium_list \u001b[32mPASSED\u001b[0m\u001b[33m [  3%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_create_ticker_batches_large_list \u001b[32mPASSED\u001b[0m\u001b[33m [  5%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_create_ticker_batches_custom_size \u001b[32mPASSED\u001b[0m\u001b[33m [  7%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_process_ticker_batch \u001b[32mPASSED\u001b[0m\u001b[33m [  8%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_process_ticker_batch_with_errors \u001b[32mPASSED\u001b[0m\u001b[33m [ 10%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_execute_strategy_concurrent_performance \u001b[32mPASSED\u001b[0m\u001b[33m [ 12%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_execute_strategy_concurrent_falls_back_to_sequential \u001b[32mPASSED\u001b[0m\u001b[33m [ 14%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_execute_strategy_concurrent_error_handling \u001b[32mPASSED\u001b[0m\u001b[33m [ 16%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_concurrent_execution_thread_safety \u001b[32mPASSED\u001b[0m\u001b[33m [ 17%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_concurrent_execution_progress_tracking \u001b[32mPASSED\u001b[0m\u001b[33m [ 19%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_empty_ticker_list_handling \u001b[32mPASSED\u001b[0m\u001b[33m [ 21%]\u001b[0m\ntests/strategies/ma_cross/test_concurrent_execution.py::TestConcurrentExecution::test_invalid_config_handling \u001b[32mPASSED\u001b[0m\u001b[33m [ 23%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossConfig::test_config_creation \u001b[31mERROR\u001b[0m\u001b[31m [ 25%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossConfig::test_config_validation \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossConfig::test_config_defaults \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_analyzer_initialization \u001b[31mERROR\u001b[0m\u001b[31m [ 30%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_fetch_data \u001b[31mERROR\u001b[0m\u001b[31m [ 32%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_calculate_sma \u001b[31mFAILED\u001b[0m\u001b[31m [ 33%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_calculate_ema \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_generate_signals \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_calculate_metrics \u001b[31mFAILED\u001b[0m\u001b[31m [ 39%]\u001b[0m\ntests/strategies/ma_cross/test_core_components.py::TestMACrossAnalyzer::test_analyze_single_ticker \u001b[31mERROR\u001b[0m\u001b[31m [ 41%]\u001b[0m\n\n==================================== ERRORS ====================================\n\u001b[31m\u001b[1m___________ ERROR at setup of TestMACrossConfig.test_config_creation ___________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:25: in sample_config\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m MACrossConfig(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\u001b[0m\n\u001b[31m\u001b[1m______ ERROR at setup of TestMACrossAnalyzer.test_analyzer_initialization ______\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:25: in sample_config\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m MACrossConfig(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\u001b[0m\n\u001b[31m\u001b[1m____________ ERROR at setup of TestMACrossAnalyzer.test_fetch_data _____________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:25: in sample_config\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m MACrossConfig(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\u001b[0m\n\u001b[31m\u001b[1m_______ ERROR at setup of TestMACrossAnalyzer.test_analyze_single_ticker _______\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:25: in sample_config\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m MACrossConfig(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\u001b[0m\n=================================== FAILURES ===================================\n\u001b[31m\u001b[1m___________________ TestMACrossConfig.test_config_validation ___________________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:77: in test_config_validation\n    \u001b[0mMACrossConfig(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\u001b[0m\n\u001b[31m\u001b[1m____________________ TestMACrossConfig.test_config_defaults ____________________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:99: in test_config_defaults\n    \u001b[0mconfig = MACrossConfig(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\u001b[0m\n\u001b[31m\u001b[1m____________________ TestMACrossAnalyzer.test_calculate_sma ____________________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:141: in test_calculate_sma\n    \u001b[0msma_20 = analyzer._calculate_ma(sample_price_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mClose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[94m20\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mSMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MACrossAnalyzer' object has no attribute '_calculate_ma'\u001b[0m\n\u001b[31m\u001b[1m____________________ TestMACrossAnalyzer.test_calculate_ema ____________________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:155: in test_calculate_ema\n    \u001b[0mema_12 = analyzer._calculate_ma(sample_price_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mClose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[94m12\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mEMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MACrossAnalyzer' object has no attribute '_calculate_ma'\u001b[0m\n\u001b[31m\u001b[1m__________________ TestMACrossAnalyzer.test_generate_signals ___________________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:174: in test_generate_signals\n    \u001b[0msignals = analyzer._generate_signals(fast_ma, slow_ma)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MACrossAnalyzer' object has no attribute '_generate_signals'\u001b[0m\n\u001b[31m\u001b[1m__________________ TestMACrossAnalyzer.test_calculate_metrics __________________\u001b[0m\n\u001b[1m\u001b[31mtests/strategies/ma_cross/test_core_components.py\u001b[0m:189: in test_calculate_metrics\n    \u001b[0mmetrics = analyzer._calculate_metrics(returns, equity_curve, num_trades=\u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'MACrossAnalyzer' object has no attribute '_calculate_metrics'\u001b[0m\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mFAILED\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossConfig::test_config_validation\u001b[0m - TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\n\u001b[31mFAILED\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossConfig::test_config_defaults\u001b[0m - TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\n\u001b[31mFAILED\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_calculate_sma\u001b[0m - AttributeError: 'MACrossAnalyzer' object has no attribute '_calculate_ma'\n\u001b[31mFAILED\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_calculate_ema\u001b[0m - AttributeError: 'MACrossAnalyzer' object has no attribute '_calculate_ma'\n\u001b[31mFAILED\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_generate_signals\u001b[0m - AttributeError: 'MACrossAnalyzer' object has no attribute '_generate_signals'\n\u001b[31mFAILED\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_calculate_metrics\u001b[0m - AttributeError: 'MACrossAnalyzer' object has no attribute '_calculate_metrics'\n\u001b[31mERROR\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossConfig::test_config_creation\u001b[0m - TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\n\u001b[31mERROR\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_analyzer_initialization\u001b[0m - TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\n\u001b[31mERROR\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_fetch_data\u001b[0m - TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\n\u001b[31mERROR\u001b[0m tests/strategies/ma_cross/test_core_components.py::\u001b[1mTestMACrossAnalyzer::test_analyze_single_ticker\u001b[0m - TypeError: AnalysisConfig.__init__() got an unexpected keyword argument 'tickers'\n\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 10 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n\u001b[31m====== \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m13 passed\u001b[0m, \u001b[33m2 deselected\u001b[0m, \u001b[33m12 warnings\u001b[0m, \u001b[31m\u001b[1m4 errors\u001b[0m\u001b[31m in 2.83s\u001b[0m\u001b[31m =======\u001b[0m\n"
    },
    {
      "description": "Concurrency Smoke Tests",
      "command": "python -m pytest tests/concurrency/test_smoke.py -v",
      "success": true,
      "output": ""
    },
    {
      "description": "Portfolio Orchestrator Tests",
      "command": "python -m pytest tests/test_portfolio_orchestrator.py -v --tb=short",
      "success": false,
      "output": "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pytest.ini\nplugins: timeout-2.4.0, cov-6.2.1, mock-3.14.1, anyio-3.7.1, asyncio-1.0.0\ntimeout: 300.0s\ntimeout method: thread\ntimeout func_only: False\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 19 items\n\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_execute_strategies \u001b[32mPASSED\u001b[0m\u001b[33m [  5%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_execute_strategies_error \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_export_results \u001b[32mPASSED\u001b[0m\u001b[31m [ 15%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_export_results_error \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_filter_and_process_portfolios \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_get_strategies \u001b[32mPASSED\u001b[0m\u001b[31m [ 31%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 36%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_initialize_configuration \u001b[32mPASSED\u001b[0m\u001b[31m [ 42%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_initialize_configuration_error \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_process_synthetic_configuration \u001b[32mPASSED\u001b[0m\u001b[31m [ 52%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_process_synthetic_configuration_error \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_run_full_workflow \u001b[32mPASSED\u001b[0m\u001b[31m [ 63%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestPortfolioOrchestrator::test_run_no_portfolios \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestTickerProcessor::test_execute_strategy \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestTickerProcessor::test_execute_strategy_with_progress \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestTickerProcessor::test_extract_synthetic_components \u001b[32mPASSED\u001b[0m\u001b[31m [ 84%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestTickerProcessor::test_format_ticker_synthetic \u001b[32mPASSED\u001b[0m\u001b[31m [ 89%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestTickerProcessor::test_initialization \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\ntests/test_portfolio_orchestrator.py::TestTickerProcessor::test_process_single_ticker \u001b[32mPASSED\u001b[0m\u001b[31m [100%]\u001b[0m\n\n=================================== FAILURES ===================================\n\u001b[31m\u001b[1m___________ TestPortfolioOrchestrator.test_execute_strategies_error ____________\u001b[0m\n\u001b[1m\u001b[31mapp/tools/error_context.py\u001b[0m:58: in error_context\n    \u001b[0m\u001b[94myield\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/orchestration/portfolio_orchestrator.py\u001b[0m:211: in _execute_strategies\n    \u001b[0mportfolios = \u001b[96mself\u001b[39;49;00m.ticker_processor.execute_strategy(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:1124: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._mock_call(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:1128: in _mock_call\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._execute_mock_call(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:1183: in _execute_mock_call\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m effect\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   Exception: Strategy failed\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mtests/test_portfolio_orchestrator.py\u001b[0m:126: in test_execute_strategies_error\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.orchestrator._execute_strategies(\u001b[96mself\u001b[39;49;00m.config, strategies)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/orchestration/portfolio_orchestrator.py\u001b[0m:197: in _execute_strategies\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m error_context(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\u001b[0m:158: in __exit__\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.gen.throw(typ, value, traceback)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/error_context.py\u001b[0m:88: in error_context\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m trading_error\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   app.strategies.ma_cross.exceptions.MACrossExecutionError: Executing SMA strategy failed: Strategy failed\u001b[0m\n\u001b[31m\u001b[1m_____________ TestPortfolioOrchestrator.test_export_results_error ______________\u001b[0m\n\u001b[1m\u001b[31mtests/test_portfolio_orchestrator.py\u001b[0m:161: in test_export_results_error\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.assertRaises(ExportError):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: ExportError not raised\u001b[0m\n\u001b[31m\u001b[1m_________ TestPortfolioOrchestrator.test_filter_and_process_portfolios _________\u001b[0m\n\u001b[1m\u001b[31mapp/tools/error_context.py\u001b[0m:58: in error_context\n    \u001b[0m\u001b[94myield\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/orchestration/portfolio_orchestrator.py\u001b[0m:264: in _filter_and_process_portfolios\n    \u001b[0mfiltered_portfolios = filtered_portfolios_df.to_dicts()\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AttributeError: 'list' object has no attribute 'to_dicts'\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mtests/test_portfolio_orchestrator.py\u001b[0m:137: in test_filter_and_process_portfolios\n    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m.orchestrator._filter_and_process_portfolios(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/orchestration/portfolio_orchestrator.py\u001b[0m:239: in _filter_and_process_portfolios\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m error_context(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\u001b[0m:158: in __exit__\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.gen.throw(typ, value, traceback)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/error_context.py\u001b[0m:88: in error_context\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m trading_error\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   app.tools.exceptions.TradingSystemError: Processing portfolios failed: 'list' object has no attribute 'to_dicts'\u001b[0m\n\u001b[31m\u001b[1m________ TestPortfolioOrchestrator.test_initialize_configuration_error _________\u001b[0m\n\u001b[1m\u001b[31mapp/tools/error_context.py\u001b[0m:58: in error_context\n    \u001b[0m\u001b[94myield\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/orchestration/portfolio_orchestrator.py\u001b[0m:142: in _initialize_configuration\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ConfigService.process_config(config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:1124: in __call__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._mock_call(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:1128: in _mock_call\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._execute_mock_call(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:1183: in _execute_mock_call\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m effect\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   ValueError: Invalid config\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mtests/test_portfolio_orchestrator.py\u001b[0m:70: in test_initialize_configuration_error\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.orchestrator._initialize_configuration(\u001b[96mself\u001b[39;49;00m.config)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/orchestration/portfolio_orchestrator.py\u001b[0m:133: in _initialize_configuration\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m error_context(\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\u001b[0m:158: in __exit__\n    \u001b[0m\u001b[96mself\u001b[39;49;00m.gen.throw(typ, value, traceback)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/error_context.py\u001b[0m:88: in error_context\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m trading_error\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   app.tools.exceptions.TradingSystemError: Initializing configuration failed: Invalid config\u001b[0m\n\u001b[31m\u001b[1m_____ TestPortfolioOrchestrator.test_process_synthetic_configuration_error _____\u001b[0m\n\u001b[1m\u001b[31mtests/test_portfolio_orchestrator.py\u001b[0m:89: in test_process_synthetic_configuration_error\n    \u001b[0m\u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.assertRaises(SyntheticTickerError):\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: SyntheticTickerError not raised\u001b[0m\n\u001b[31m\u001b[1m___________ TestTickerProcessor.test_execute_strategy_with_progress ____________\u001b[0m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/unittest/mock.py\u001b[0m:950: in assert_called_once_with\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(msg)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Expected 'set_total_steps' to be called once. Called 0 times.\u001b[0m\n\n\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n\u001b[1m\u001b[31mtests/test_portfolio_orchestrator.py\u001b[0m:289: in test_execute_strategy_with_progress\n    \u001b[0mmock_progress.set_total_steps.assert_called_once_with(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   AssertionError: Expected 'set_total_steps' to be called once. Called 0 times.\u001b[0m\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mFAILED\u001b[0m tests/test_portfolio_orchestrator.py::\u001b[1mTestPortfolioOrchestrator::test_execute_strategies_error\u001b[0m - app.strategies.ma_cross.exceptions.MACrossExecutionError: Executing SMA strategy failed: Strategy failed\n\u001b[31mFAILED\u001b[0m tests/test_portfolio_orchestrator.py::\u001b[1mTestPortfolioOrchestrator::test_export_results_error\u001b[0m - AssertionError: ExportError not raised\n\u001b[31mFAILED\u001b[0m tests/test_portfolio_orchestrator.py::\u001b[1mTestPortfolioOrchestrator::test_filter_and_process_portfolios\u001b[0m - app.tools.exceptions.TradingSystemError: Processing portfolios failed: 'list' object has no attribute 'to_dicts'\n\u001b[31mFAILED\u001b[0m tests/test_portfolio_orchestrator.py::\u001b[1mTestPortfolioOrchestrator::test_initialize_configuration_error\u001b[0m - app.tools.exceptions.TradingSystemError: Initializing configuration failed: Invalid config\n\u001b[31mFAILED\u001b[0m tests/test_portfolio_orchestrator.py::\u001b[1mTestPortfolioOrchestrator::test_process_synthetic_configuration_error\u001b[0m - AssertionError: SyntheticTickerError not raised\n\u001b[31mFAILED\u001b[0m tests/test_portfolio_orchestrator.py::\u001b[1mTestTickerProcessor::test_execute_strategy_with_progress\u001b[0m - AssertionError: Expected 'set_total_steps' to be called once. Called 0 times.\n\u001b[31m================== \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m13 passed\u001b[0m, \u001b[33m12 warnings\u001b[0m\u001b[31m in 2.59s\u001b[0m\u001b[31m ===================\u001b[0m\n"
    },
    {
      "description": "Export Integration Tests",
      "command": "python -m pytest tests/test_export_integration.py -v --tb=short",
      "success": false,
      "output": "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pytest.ini\nplugins: timeout-2.4.0, cov-6.2.1, mock-3.14.1, anyio-3.7.1, asyncio-1.0.0\ntimeout: 300.0s\ntimeout method: thread\ntimeout func_only: False\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 9 items\n\ntests/test_export_integration.py::TestExportIntegration::test_orchestrator_with_new_export \u001b[32mPASSED\u001b[0m\u001b[33m [ 11%]\u001b[0m\ntests/test_export_integration.py::TestExportIntegration::test_orchestrator_backward_compatibility \u001b[32mPASSED\u001b[0m\u001b[33m [ 22%]\u001b[0m\ntests/test_export_integration.py::TestExportIntegration::test_export_csv_adapter \u001b[32mPASSED\u001b[0m\u001b[33m [ 33%]\u001b[0m\ntests/test_export_integration.py::TestExportIntegration::test_migrate_to_export_manager \u001b[32mPASSED\u001b[0m\u001b[33m [ 44%]\u001b[0m\ntests/test_export_integration.py::TestExportIntegration::test_export_with_date_subdirectory \u001b[32mPASSED\u001b[0m\u001b[33m [ 55%]\u001b[0m\ntests/test_export_integration.py::TestExportIntegration::test_synthetic_ticker_export \u001b[32mPASSED\u001b[0m\u001b[33m [ 66%]\u001b[0m\ntests/test_export_integration.py::TestExportIntegration::test_batch_export_integration \u001b[32mPASSED\u001b[0m\u001b[33m [ 77%]\u001b[0m\ntests/test_export_integration.py::TestExportSystemRealFiles::test_real_csv_export \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\ntests/test_export_integration.py::TestExportSystemRealFiles::test_real_json_export \u001b[32mPASSED\u001b[0m\u001b[31m [100%]\u001b[0m\n\n=================================== FAILURES ===================================\n\u001b[31m\u001b[1m________________ TestExportSystemRealFiles.test_real_csv_export ________________\u001b[0m\n\u001b[1m\u001b[31mtests/test_export_integration.py\u001b[0m:318: in test_real_csv_export\n    \u001b[0m\u001b[94massert\u001b[39;49;00m df_read[\u001b[33m\"\u001b[39;49;00m\u001b[33mticker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[94m0\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33mBTC-USD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/polars/dataframe/frame.py\u001b[0m:1395: in __getitem__\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m get_df_item_by_key(\u001b[96mself\u001b[39;49;00m, key)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/polars/_utils/getitem.py\u001b[0m:163: in get_df_item_by_key\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m df.get_column(key)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/polars/dataframe/frame.py\u001b[0m:8615: in get_column\n    \u001b[0m\u001b[94mreturn\u001b[39;49;00m wrap_s(\u001b[96mself\u001b[39;49;00m._df.get_column(name))\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   polars.exceptions.ColumnNotFoundError: \"ticker\" not found\u001b[0m\n------------------------------ Captured log call -------------------------------\n\u001b[32mINFO    \u001b[0m root:formats.py:87 2 rows exported to /private/var/folders/wv/6psymz1153j041rh26kgql4m0000gn/T/pytest-of-colemorton/pytest-0/test_real_csv_export0/export_test/csv/test_data/real_test.csv\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mFAILED\u001b[0m tests/test_export_integration.py::\u001b[1mTestExportSystemRealFiles::test_real_csv_export\u001b[0m - polars.exceptions.ColumnNotFoundError: \"ticker\" not found\n\u001b[31m=================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m8 passed\u001b[0m, \u001b[33m12 warnings\u001b[0m\u001b[31m in 2.60s\u001b[0m\u001b[31m ===================\u001b[0m\n"
    },
    {
      "description": "Strategy Integration Tests",
      "command": "python -m pytest tests/test_strategy_integration.py -v --tb=short",
      "success": false,
      "output": "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pytest.ini\nplugins: timeout-2.4.0, cov-6.2.1, mock-3.14.1, anyio-3.7.1, asyncio-1.0.0\ntimeout: 300.0s\ntimeout method: thread\ntimeout func_only: False\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 7 items\n\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_sma \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_ema \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_execute_single_strategy_with_factory \u001b[32mPASSED\u001b[0m\u001b[31m [ 42%]\u001b[0m\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_factory_available_strategies \u001b[32mPASSED\u001b[0m\u001b[31m [ 57%]\u001b[0m\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_backward_compatibility_with_string_parameter \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_config_override_strategy_type \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\ntests/test_strategy_integration.py::TestFactoryIntegrationWithExistingCode::test_error_handling_with_invalid_strategy \u001b[32mPASSED\u001b[0m\u001b[31m [100%]\u001b[0m\n\n=================================== FAILURES ===================================\n\u001b[31m\u001b[1m_ TestFactoryIntegrationWithExistingCode.test_calculate_ma_and_signals_with_sma _\u001b[0m\n\u001b[1m\u001b[31mtests/test_strategy_integration.py\u001b[0m:52: in test_calculate_ma_and_signals_with_sma\n    \u001b[0mresult = calculate_ma_and_signals(data, \u001b[94m10\u001b[39;49;00m, \u001b[94m20\u001b[39;49;00m, config, log)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/calculate_ma_and_signals.py\u001b[0m:41: in calculate_ma_and_signals\n    \u001b[0mresult = strategy.calculate(data, short_window, long_window, config, log)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/strategy/concrete.py\u001b[0m:53: in calculate\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   ValueError: Invalid data\u001b[0m\n\u001b[31m\u001b[1m_ TestFactoryIntegrationWithExistingCode.test_calculate_ma_and_signals_with_ema _\u001b[0m\n\u001b[1m\u001b[31mtests/test_strategy_integration.py\u001b[0m:81: in test_calculate_ma_and_signals_with_ema\n    \u001b[0mresult = calculate_ma_and_signals(data, \u001b[94m12\u001b[39;49;00m, \u001b[94m26\u001b[39;49;00m, config, log, \u001b[33m\"\u001b[39;49;00m\u001b[33mEMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/calculate_ma_and_signals.py\u001b[0m:41: in calculate_ma_and_signals\n    \u001b[0mresult = strategy.calculate(data, short_window, long_window, config, log)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/strategy/concrete.py\u001b[0m:134: in calculate\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   ValueError: Invalid data\u001b[0m\n\u001b[31m\u001b[1m_ TestFactoryIntegrationWithExistingCode.test_backward_compatibility_with_string_parameter _\u001b[0m\n\u001b[1m\u001b[31mtests/test_strategy_integration.py\u001b[0m:170: in test_backward_compatibility_with_string_parameter\n    \u001b[0mresult = calculate_ma_and_signals(data, \u001b[94m20\u001b[39;49;00m, \u001b[94m50\u001b[39;49;00m, config, log, \u001b[33m\"\u001b[39;49;00m\u001b[33mSMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/calculate_ma_and_signals.py\u001b[0m:41: in calculate_ma_and_signals\n    \u001b[0mresult = strategy.calculate(data, short_window, long_window, config, log)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/strategy/concrete.py\u001b[0m:53: in calculate\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   ValueError: Invalid data\u001b[0m\n\u001b[31m\u001b[1m__ TestFactoryIntegrationWithExistingCode.test_config_override_strategy_type ___\u001b[0m\n\u001b[1m\u001b[31mtests/test_strategy_integration.py\u001b[0m:196: in test_config_override_strategy_type\n    \u001b[0mresult = calculate_ma_and_signals(data, \u001b[94m12\u001b[39;49;00m, \u001b[94m26\u001b[39;49;00m, config, log, \u001b[33m\"\u001b[39;49;00m\u001b[33mSMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/calculate_ma_and_signals.py\u001b[0m:41: in calculate_ma_and_signals\n    \u001b[0mresult = strategy.calculate(data, short_window, long_window, config, log)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mapp/tools/strategy/concrete.py\u001b[0m:134: in calculate\n    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n\u001b[1m\u001b[31mE   ValueError: Invalid data\u001b[0m\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mFAILED\u001b[0m tests/test_strategy_integration.py::\u001b[1mTestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_sma\u001b[0m - ValueError: Invalid data\n\u001b[31mFAILED\u001b[0m tests/test_strategy_integration.py::\u001b[1mTestFactoryIntegrationWithExistingCode::test_calculate_ma_and_signals_with_ema\u001b[0m - ValueError: Invalid data\n\u001b[31mFAILED\u001b[0m tests/test_strategy_integration.py::\u001b[1mTestFactoryIntegrationWithExistingCode::test_backward_compatibility_with_string_parameter\u001b[0m - ValueError: Invalid data\n\u001b[31mFAILED\u001b[0m tests/test_strategy_integration.py::\u001b[1mTestFactoryIntegrationWithExistingCode::test_config_override_strategy_type\u001b[0m - ValueError: Invalid data\n\u001b[31m=================== \u001b[31m\u001b[1m4 failed\u001b[0m, \u001b[32m3 passed\u001b[0m, \u001b[33m12 warnings\u001b[0m\u001b[31m in 2.60s\u001b[0m\u001b[31m ===================\u001b[0m\n"
    },
    {
      "description": "API Unit Tests",
      "command": "python -m pytest tests/api/ -v --tb=short -k not server",
      "success": false,
      "output": "\u001b[1m============================= test session starts ==============================\u001b[0m\nplatform darwin -- Python 3.11.9, pytest-8.3.5, pluggy-1.6.0 -- /Users/colemorton/.pyenv/versions/3.11.9/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/colemorton/Projects/trading\nconfigfile: pytest.ini\nplugins: timeout-2.4.0, cov-6.2.1, mock-3.14.1, anyio-3.7.1, asyncio-1.0.0\ntimeout: 300.0s\ntimeout method: thread\ntimeout func_only: False\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function\n\u001b[1mcollecting ... \u001b[0mcollected 89 items / 10 errors\n\n==================================== ERRORS ====================================\n\u001b[31m\u001b[1m_______________ ERROR collecting tests/api/test_all_features.py ________________\u001b[0m\n\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:176: in exec_module\n    source_stat, co = _rewrite_test(fn, self.config)\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:356: in _rewrite_test\n    tree = ast.parse(source, filename=strfn)\n../../.pyenv/versions/3.11.9/lib/python3.11/ast.py:50: in parse\n    return compile(source, filename, mode, flags,\nE     File \"/Users/colemorton/Projects/trading/tests/api/test_all_features.py\", line 123\nE       f\"\u2713 Total portfolios: {\nE       ^\nE   SyntaxError: unterminated string literal (detected at line 123)\u001b[0m\n\u001b[31m\u001b[1m__________ ERROR collecting tests/api/test_async_progress_tracking.py __________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_async_progress_tracking.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_async_progress_tracking.py:17: in <module>\n    from app.api.models.ma_cross import MACrossRequest\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m__________ ERROR collecting tests/api/test_ma_cross_data_pipeline.py ___________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_data_pipeline.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_data_pipeline.py:13: in <module>\n    from app.api.models.ma_cross import PortfolioMetrics\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m_____________ ERROR collecting tests/api/test_ma_cross_enhanced.py _____________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_enhanced.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_enhanced.py:14: in <module>\n    from app.api.models.ma_cross import (\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m___________ ERROR collecting tests/api/test_ma_cross_integration.py ____________\u001b[0m\n\u001b[31m../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:176: in exec_module\n    source_stat, co = _rewrite_test(fn, self.config)\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:356: in _rewrite_test\n    tree = ast.parse(source, filename=strfn)\n../../.pyenv/versions/3.11.9/lib/python3.11/ast.py:50: in parse\n    return compile(source, filename, mode, flags,\nE     File \"/Users/colemorton/Projects/trading/tests/api/test_ma_cross_integration.py\", line 58\nE       f\"- Total portfolios analyzed: {\nE       ^\nE   SyntaxError: unterminated string literal (detected at line 58)\u001b[0m\n\u001b[31m\u001b[1m______________ ERROR collecting tests/api/test_ma_cross_router.py ______________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_router.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_router.py:13: in <module>\n    from app.api.models.ma_cross import (\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m________ ERROR collecting tests/api/test_ma_cross_schema_compliance.py _________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_schema_compliance.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_schema_compliance.py:17: in <module>\n    from app.api.models.ma_cross import MACrossRequest\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m_____________ ERROR collecting tests/api/test_ma_cross_service.py ______________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_service.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_service.py:12: in <module>\n    from app.api.models.ma_cross import (\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m__________ ERROR collecting tests/api/test_ma_cross_service_fixed.py ___________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_service_fixed.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_service_fixed.py:11: in <module>\n    from app.api.models.ma_cross import (\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[31m\u001b[1m__________ ERROR collecting tests/api/test_ma_cross_service_simple.py __________\u001b[0m\n\u001b[31mImportError while importing test module '/Users/colemorton/Projects/trading/tests/api/test_ma_cross_service_simple.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/python.py:493: in importtestmodule\n    mod = import_path(\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/pathlib.py:587: in import_path\n    importlib.import_module(module_name)\n../../.pyenv/versions/3.11.9/lib/python3.11/importlib/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n<frozen importlib._bootstrap>:1204: in _gcd_import\n    ???\n<frozen importlib._bootstrap>:1176: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:690: in _load_unlocked\n    ???\n../../.pyenv/versions/3.11.9/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:185: in exec_module\n    exec(co, module.__dict__)\ntests/api/test_ma_cross_service_simple.py:11: in <module>\n    from app.api.models.ma_cross import (\nE   ModuleNotFoundError: No module named 'app.api.models.ma_cross'\u001b[0m\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mERROR\u001b[0m tests/api/test_all_features.py\n\u001b[31mERROR\u001b[0m tests/api/test_async_progress_tracking.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_data_pipeline.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_enhanced.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_integration.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_router.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_schema_compliance.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_service.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_service_fixed.py\n\u001b[31mERROR\u001b[0m tests/api/test_ma_cross_service_simple.py\n\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 10 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n\u001b[31m======================= \u001b[33m12 warnings\u001b[0m, \u001b[31m\u001b[1m10 errors\u001b[0m\u001b[31m in 1.01s\u001b[0m\u001b[31m ========================\u001b[0m\n"
    }
  ]
}
