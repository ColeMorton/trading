name: MA Cross Module Tests

on:
  push:
    branches: [main, develop]
    paths:
      - 'app/ma_cross/**'
      - 'app/tools/**'
      - 'tests/**'
      - '.github/workflows/ma_cross_tests.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'app/ma_cross/**'
      - 'app/tools/**'
      - 'tests/**'
  schedule:
    # Run regression tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - regression
          - smoke

# Version configuration: Uses defaults from .github/actions/setup-python-poetry/action.yml
# See .versions file for canonical version reference
env:
  PYTHON_VERSION: '3.11'

jobs:
  test:
    name: Test Suite
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-suite:
          - ${{ github.event.inputs.test_suite == 'all' && 'smoke' || github.event.inputs.test_suite || 'smoke' }}
          - ${{ github.event.inputs.test_suite == 'all' && 'unit' || '' }}
          - ${{ github.event.inputs.test_suite == 'all' && 'integration' || '' }}
          - ${{ github.event.inputs.test_suite == 'all' && 'regression' || '' }}
        exclude:
          - test-suite: ''
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python with Poetry
        uses: ./.github/actions/setup-python-poetry
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # poetry-version uses default from action (see .versions file)
          dependency-groups: 'all'
          cache-key-suffix: 'ma-cross-${{ matrix.test-suite }}'

      - name: Lint code
        run: |
          poetry run ruff check app/ma_cross/ app/tools/ --output-format=github
        continue-on-error: true

      - name: Type check
        run: |
          poetry run mypy app/ma_cross/ app/tools/ --ignore-missing-imports
        continue-on-error: true

      - name: Run ${{ matrix.test-suite }} tests
        run: |
          # Run tests directly with pytest instead of custom runner
          if [ "${{ matrix.test-suite }}" = "smoke" ]; then
            poetry run pytest tests/strategies/ma_cross/ -v -m "smoke" --tb=short
          elif [ "${{ matrix.test-suite }}" = "unit" ]; then
            poetry run pytest tests/strategies/ma_cross/ -v -m "unit" --cov=app/ma_cross --cov-report=xml --cov-report=term-missing -n auto
          elif [ "${{ matrix.test-suite }}" = "integration" ]; then
            poetry run pytest tests/strategies/ma_cross/ -v -m "integration" --tb=short
          elif [ "${{ matrix.test-suite }}" = "regression" ]; then
            poetry run pytest tests/strategies/ma_cross/ -v -m "regression" --tb=short
          fi
        timeout-minutes: 30

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ma-cross-test-results-${{ matrix.test-suite }}
          path: |
            .coverage
            coverage.xml
            .pytest_cache/
          retention-days: 30

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.test-suite == 'unit' || matrix.test-suite == 'all'
        with:
          file: ./coverage.xml
          flags: ma-cross
          name: codecov-ma-cross
          fail_ci_if_error: false

  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_suite == 'performance' || github.event.inputs.test_suite == 'all'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python with Poetry
        uses: ./.github/actions/setup-python-poetry
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # poetry-version uses default from action (see .versions file)
          dependency-groups: 'all'
          cache-key-suffix: 'ma-cross-perf'

      - name: Run performance benchmarks
        run: |
          poetry run pytest tests/strategies/ma_cross/ -v -m "performance" --tb=short --benchmark-only
        timeout-minutes: 45

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: ma-cross-performance-results
          path: |
            .pytest_cache/
            .benchmarks/
          retention-days: 90

  e2e:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    # E2E tests are skipped for MA Cross module - no E2E tests currently exist
    # The module has comprehensive unit (31 tests) and integration (39 tests) coverage
    # E2E tests would require full-stack Docker environment and real market data
    # If E2E tests are added in the future, update this condition to enable the job
    if: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python with Poetry
        uses: ./.github/actions/setup-python-poetry
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # poetry-version uses default from action (see .versions file)
          dependency-groups: 'all'
          cache-key-suffix: 'ma-cross-e2e'

      - name: Create test directories
        run: |
          mkdir -p csv/portfolios csv/portfolios_filtered csv/strategies
          mkdir -p logs

      - name: Run end-to-end tests
        run: |
          poetry run pytest tests/strategies/ma_cross/ -v -m "e2e" --tb=short
        timeout-minutes: 20

      - name: Upload e2e results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ma-cross-e2e-results
          path: |
            .pytest_cache/
            logs/
          retention-days: 30

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test, performance, e2e]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate summary
        run: |
          echo "# MA Cross Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check test results
          if [ "${{ needs.test.result }}" == "success" ]; then
            echo "✅ **Unit/Integration Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Unit/Integration Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance.result }}" == "success" ]; then
            echo "✅ **Performance Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.performance.result }}" == "skipped" ]; then
            echo "⏭️ **Performance Tests**: SKIPPED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Performance Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.e2e.result }}" == "success" ]; then
            echo "✅ **End-to-End Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.e2e.result }}" == "skipped" ]; then
            echo "⏭️ **End-to-End Tests**: SKIPPED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **End-to-End Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Test results and coverage reports are available in the artifacts section" >> $GITHUB_STEP_SUMMARY
          echo "- Performance benchmarks are retained for 90 days" >> $GITHUB_STEP_SUMMARY
          echo "- All other test results are retained for 30 days" >> $GITHUB_STEP_SUMMARY
